{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly.offline import plot\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from utils import save_plotly_figure_as_html\n",
    "from ex_01_read_data import get_welding_data\n",
    "from ex_03_feature_extraction import extract_features\n",
    "from ex_04_my_kmeans import MyKMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"data/Welding/data.csv\")\n",
    "plot_path = Path(\"plots/ex_04\")\n",
    "plot_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "data, labels, exp_ids = get_welding_data(data_path, n_samples=5_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Optimal Cluster Selection\n",
    "\n",
    "Implement the elbow method to find the optimal number of clusters for the raw time series data using your implement of the kmeans algorithm (MyKMeans).\n",
    "\n",
    "1. Calculate the distortion (inertia) for different values of k (1 to 10)\n",
    "2. Plot the results to identify the \"elbow point\" with matplotlib where adding more clusters produces diminishing returns\n",
    "3. This will help determine the most appropriate number of clusters for our welding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KMeans fitting:   4%|‚ñç         | 2/50 [00:00<00:01, 40.22it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m     plt.show()\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43melbow_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36melbow_method\u001b[39m\u001b[34m(X, max_clusters, distance_metric)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ks:\n\u001b[32m     14\u001b[39m     model = MyKMeans(\n\u001b[32m     15\u001b[39m         k=k,\n\u001b[32m     16\u001b[39m         max_iter=\u001b[32m50\u001b[39m,\n\u001b[32m     17\u001b[39m         distance_metric=distance_metric,\n\u001b[32m     18\u001b[39m         init_method=\u001b[33m'\u001b[39m\u001b[33mkmeans++\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     19\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     inertias.append(model.inertia_)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Plot the elbow curve\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aml/AMLProjectPart1/ex_04_my_kmeans.py:68\u001b[39m, in \u001b[36mMyKMeans.fit\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput data must be a numpy array or pandas DataFrame\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# Initialize centroids\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28mself\u001b[39m.centroids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize_centroids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m prev_centroids = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Iterate the K-means clustering process\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aml/AMLProjectPart1/ex_04_my_kmeans.py:135\u001b[39m, in \u001b[36mMyKMeans._initialize_centroids\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# 2) For each subsequent centroid\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.k):\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# Compute distances to nearest existing centroid\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     dist_sq = np.min(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m**\u001b[32m2\u001b[39m, axis=\u001b[32m1\u001b[39m)\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# Probability proportional to squared distance\u001b[39;00m\n\u001b[32m    137\u001b[39m     prob = dist_sq / np.sum(dist_sq)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/aml/AMLProjectPart1/ex_04_my_kmeans.py:153\u001b[39m, in \u001b[36mMyKMeans._compute_distance\u001b[39m\u001b[34m(self, x, centroids)\u001b[39m\n\u001b[32m    150\u001b[39m distances = np.zeros((n_samples, \u001b[38;5;28mself\u001b[39m.k))\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.k):\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     c = \u001b[43mcentroids\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.distance_metric == \u001b[33m\"\u001b[39m\u001b[33meuclidean\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    155\u001b[39m         \u001b[38;5;66;03m# Flatten if time-series\u001b[39;00m\n\u001b[32m    156\u001b[39m         distances[:, idx] = np.linalg.norm(x - c, axis=\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, x.ndim)))\n",
      "\u001b[31mIndexError\u001b[39m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "def elbow_method(X, max_clusters=10, distance_metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Apply the elbow method to find the optimal number of clusters.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix or raw data (numpy array or DataFrame)\n",
    "        max_clusters: Maximum number of clusters to try\n",
    "        distance_metric: Distance metric to use for MyKMeans\n",
    "    \"\"\"\n",
    "    ks = range(1, max_clusters + 1)\n",
    "    inertias = []\n",
    "\n",
    "    for k in ks:\n",
    "        model = MyKMeans(\n",
    "            k=k,\n",
    "            max_iter=50,\n",
    "            distance_metric=distance_metric,\n",
    "            init_method='kmeans++'\n",
    "        )\n",
    "        model.fit(X)\n",
    "        inertias.append(model.inertia_)\n",
    "\n",
    "    # Plot the elbow curve\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(ks, inertias, marker='o', linestyle='-')\n",
    "    plt.xticks(ks)\n",
    "    plt.xlabel('Number of Clusters (k)')\n",
    "    plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "    plt.title('Elbow Method for MyKMeans on Welding Data')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # Save plot\n",
    "    plt.savefig(plot_path / 'elbow_method.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "elbow_method(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.3: Clustering with MyKMeans\n",
    "\n",
    "### Based on the elbow curve above, explain your choice of k:\n",
    "\n",
    "1. What does the shape of the elbow curve tell you about the underlying data structure?\n",
    "2. Why did you select this specific number of clusters?\n",
    "   - Consider the plot and the elbow method to justify your choice\n",
    "   - How might this choice affect the interpretability of the resulting clusters?\n",
    "\n",
    "### KMeans with euclidean distance and dtw distance\n",
    "1. run K means with you selected k \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.4: Cluster Visualization\n",
    "\n",
    "Plot the mean time series (centroids) for each cluster to visualize and understand the patterns.\n",
    "\n",
    "Remember that our welding data has both current and voltage measurements over time (shape: n_samples, sequence_length, features). For each cluster:\n",
    "1. Plot the average current pattern\n",
    "2. Plot the average voltage pattern\n",
    "3. Look for distinctive characteristics in each cluster that might relate to welding quality\n",
    "\n",
    "This visualization will help identify what makes each cluster unique in terms of temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5: Feature-Based Clustering\n",
    "\n",
    "Now we'll use the engineered features extracted in Exercise 3 instead of raw time series data. Therefore, you can use the euclidean distance metric.\n",
    "\n",
    "1. Load your extracted features from exercise 3 \n",
    "2. Split them into data and labels\n",
    "3. Scale the data for better clustering performance\n",
    "4. Apply the elbow method again to determine the optimal number of clusters for the feature-based approach\n",
    "5. Compare this result with the clustering of raw time series data. Consider why the optimal k might differ between the two approaches:\n",
    "   - Do engineered features represent the data differently?\n",
    "   - Which approach might better capture the relevant patterns for quality assessment?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the data\n",
    "features = extract_features(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.6: Advanced 3D Visualization\n",
    "\n",
    "Visualize the clustering results using interactive 3D plots with Plotly.\n",
    "\n",
    "1. Use PCA to reduce the dimensionality of our feature space to 3 components\n",
    "2. Create two visualizations:\n",
    "   - Points colored by assigned cluster\n",
    "   - Same points colored by actual quality labels\n",
    "3. Include the explained variance for each principal component in the axis labels\n",
    "4. Save the figures to the plot_path\n",
    "\n",
    "\n",
    "This visualization will help us understand how well our clustering approach aligns with the known quality designations.\n",
    "\n",
    "#### Note:\n",
    "- You can use the following links to find more information about the PCA:\n",
    "   - https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "   - https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1_save_path = save_plotly_figure_as_html(fig1, output_dir / f'clusters_3d_{n_clusters}_clusters')\n",
    "fig2_save_path = save_plotly_figure_as_html(fig2, output_dir / f'quality_3d_{n_clusters}_clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.7: Analysis and Interpretation\n",
    "\n",
    "Analyze the clustering results in depth and discuss their implications:\n",
    "\n",
    "1. Cluster separation:\n",
    "   - How well do clusters separate different quality levels?\n",
    "   - What is the Adjusted Rand Index between clusters and quality labels?\n",
    "   - Are there clusters that predominantly contain good or bad welds?\n",
    "\n",
    "2. Feature importance:\n",
    "   - Which features seem most important for distinguishing clusters?\n",
    "   - How does the PCA visualization help us understand the data structure?\n",
    "\n",
    "3. Process insights:\n",
    "   - What insights could these clusters provide for improving the welding process?\n",
    "   - Could certain clusters identify specific types of welding issues?\n",
    "\n",
    "4. Limitations:\n",
    "   - What are the limitations of using clustering for quality assessment?\n",
    "   - How might the approach be improved in future iterations?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
